---
title: Implementing AI Search for Historical Messages Using LanceDB
description: How to implement a distributed Vector Service based on LanceDB + S3 Express
image: https://fal.media/files/lion/SFhARLgEAkLIy1oIaOrSW_image.webp
date: '2025-01-08'
---

## 1 Why Do We Need Historical Message Search

The main goal is to quickly retrieve important search records from the past, swiftly locate previously searched technical documents and solutions, avoid repeated searches, and improve efficiency:

-   For example, you can quickly search for the code snippets provided by MemFree in the past.
-   For example, you can quickly find the documents generated by MemFree before.
-   For example, you can quickly search for the Shell commands generated by MemFree earlier.

## 2 Why Do We Need AI Search

AI search, also known as vector search or semantic search, has the following advantages:

1. **AI search has cross-language understanding capabilities**: Inputting "AI Semantic Search" can still yield relevant results for "AI intelligent search."
2. **Intelligent error correction**: You can search for results related to "MemFree" by inputting "MemFrre."
3. **Semantic understanding capabilities**: Searching for "搜索引擎" can also return content related to "查询工具."
4. **Context awareness**: Searching for "apple" can intelligently differentiate between discussing the fruit or the tech company.

## 3 How to Implement AI Search

Refer to [Hybrid AI Search 2 - How to Build a Serverless Vector Search Using LanceDB](https://www.memfree.me/blog/serverless-vector-search-lancedb) and [Hybrid AI Search 3 - Complete Tech Stack](https://www.memfree.me/blog/hybrid-ai-search-tech-stack). Below are the key components and technology choices for implementing AI search:

-   **Vector Database**: Since MemFree has previously used LanceDB as a vector database to support vector search, LanceDB will also be used for historical messages.
-   **Embedding Model**: Using OpenAI's text-embedding-3-large Model
-   **Data Storage**: S3 Express
-   **AI Search Index Service Deployment**: Flyio nodes, reasons can be found in [Advantages of Deploying on Fly.io](https://www.memfree.me/docs/deploy-searxng-fly-io)
-   **AI Search Query Service Deployment**: AWS EC2 nodes. The main reason is to reduce network latency with S3 Express.

## 4 How to Implement Distributed Vector Service Based on LanceDB + S3 Express

![memfree-vector-service-lancedb](https://image.memfree.me/1736333256374-memfree-vector-service-lancedb.png)

### 4.1 S3 Does Not Support Distributed Multi-process Concurrent Writes

Since S3 does not provide native atomic renaming or conditional write operations, LanceDB recommends using external storage like DynamoDB or Redis with transaction guarantees to avoid distributed concurrent writes. However, this approach is obviously costly and maintenance can be cumbersome.

My solution is as follows:

1. **One User One Table**: Each user's data is stored in a separate table.
2. **Single Node Writes**: Write requests from the same user are handled by only one writing node at any given moment.

This allows for distributed writing for multiple users. For a single user, since LanceDB has good batch write performance, one node can completely cover it.

### 4.2 Read-Write Separation

-   **Query Latency Optimization**: To minimize query latency, query nodes are deployed in the same AWS region as S3 Express. On AWS EC2, query latency is around 150 ms, while on Fly.io or Vercel, even if in the same region as S3 Express, query latency can reach 450-500 ms.
-   **Separation of Query and Import**: Query and import operations are independent of each other, avoiding mutual interference.
-   **Import Elasticity**: Import operations require greater elasticity, and Fly.io's auto-scaling feature is very suitable for this need.

### 4.3 Consistency of Distributed Metadata

Memfree will utilize Redis to record metadata related to building indexes:

-   Status and progress information for each user's index building.
-   The number of import attempts for each user's table, which is used to promptly compact when the version number is sufficient.

## 5 Optimization Points for LanceDB Search Query Performance

1. **Batch Writes**: Write as much data as possible at once. When MemFree builds indexes for historical messages, it concurrently embeds dozens of messages and then appends all the obtained data to LanceDB at once.
2. **Timely Compact**: Use Redis to control the timing of compact operations.
3. **Column Pruning**: Only select necessary fields to reduce IO operations.
4. **Query Node Deployment Optimization**: Query nodes are deployed in the same region as storage nodes.
5. **Hardware Configuration Optimization**: If costs allow, increase CPU and memory configurations for query nodes.

## 6 MemFree Vector Search Source Code

[All source code for MemFree Vector Search has been open-sourced](https://github.com/memfreeme/memfree). Please give us a star, thank you.
